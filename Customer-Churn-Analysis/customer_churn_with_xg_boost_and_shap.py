# -*- coding: utf-8 -*-
"""CUSTOMER CHURN WITH XG BOOST AND SHAP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6o8YEpKIIVeb4FiALgP8D61WEufHL27
"""

"""
Complete Customer Churn Analysis Pipeline with XGBoost and SHAP
This script performs EDA, feature engineering, feature selection, and SHAP analysis.
Includes: Beeswarm, Bar, Dependence, Decision, Waterfall, and Force Plots.
"""

import os
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import shap
import warnings
warnings.filterwarnings('ignore')

# Create necessary directories
os.makedirs('Data', exist_ok=True)
os.makedirs('Results', exist_ok=True)

# ================================
# STEP 1: LOAD DATA
# ================================
print("="*50)
print("STEP 1: LOADING DATA")
print("="*50)

# Define the URL to the dataset
url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"

# Download and Load directly into DataFrame
print(f"Downloading dataset from: {url}")
df = pd.read_csv(url)

# Quick Validation
print(f"\nDataset Shape: {df.shape}")
print("\nFirst 5 rows:")
print(df.head())

# ================================
# STEP 2: BASIC INFO
# ================================
print("\n" + "="*50)
print("STEP 2: BASIC INFORMATION")
print("="*50)

print(f"\nDataset Shape: {df.shape}")
print(f"\nData Types:\n{df.dtypes}")
print(f"\nBasic Statistics:\n{df.describe()}")

# ================================
# STEP 3: CHECK DATA QUALITY
# ================================
print("\n" + "="*50)
print("STEP 3: DATA QUALITY CHECKS")
print("="*50)

# Check duplicates
duplicates = df.duplicated().sum()
print(f"\nDuplicate rows: {duplicates}")

# Check missing values
missing = df.isnull().sum()
print(f"\nMissing values:\n{missing[missing > 0]}")

# ================================
# STEP 4: HANDLE MISSING VALUES
# ================================
print("\n" + "="*50)
print("STEP 4: HANDLING MISSING VALUES")
print("="*50)

# Convert TotalCharges to numeric (Fixed column name)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
print(f"\nTotalCharges converted to numeric")

# Check rows with missing TotalCharges
missing_charges = df[df['TotalCharges'].isna()]
print(f"\nRows with missing TotalCharges: {len(missing_charges)}")
# Note: 'tenure' is lowercase in raw dataset
if not missing_charges.empty:
    print(f"Tenure values for missing rows:\n{missing_charges['tenure'].value_counts()}")

# Drop rows with missing TotalCharges (all have tenure = 0)
df = df.dropna(subset=['TotalCharges'])
print(f"\nAfter dropping missing values: {df.shape}")

# ================================
# STEP 5: REMOVE ID COLUMNS
# ================================
print("\n" + "="*50)
print("STEP 5: REMOVING ID COLUMNS")
print("="*50)

# Remove ID columns
id_columns = ['customerID'] # 'LoyaltyID' does not exist in this specific CSV
cols_to_drop = [col for col in id_columns if col in df.columns]
if cols_to_drop:
    df = df.drop(columns=cols_to_drop)
    print(f"Removed columns: {cols_to_drop}")
print(f"Current shape: {df.shape}")

# ================================
# STEP 6: EXPLORATORY DATA ANALYSIS
# ================================
print("\n" + "="*50)
print("STEP 6: EXPLORATORY DATA ANALYSIS")
print("="*50)

# Separate numerical and categorical columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"\nNumerical columns ({len(numerical_cols)}): {numerical_cols}")
print(f"\nCategorical columns ({len(categorical_cols)}): {categorical_cols}")

# Target distribution
print(f"\nTarget Distribution:")
print(df['Churn'].value_counts())
print(f"\nChurn Rate: {df['Churn'].value_counts(normalize=True)['Yes']:.2%}")

# ================================
# STEP 7: VISUALIZATIONS
# ================================
print("\n" + "="*50)
print("STEP 7: CREATING VISUALIZATIONS")
print("="*50)

# Numerical features distribution
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for idx, col in enumerate(numerical_cols):
    if idx < 3:
        axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7)
        axes[idx].set_title(f'{col} Distribution')
        axes[idx].set_xlabel(col)
        axes[idx].set_ylabel('Frequency')
plt.tight_layout()
plt.savefig('numerical_distributions.png', dpi=300, bbox_inches='tight')
print("Saved: numerical_distributions.png")
plt.close()

# Categorical features by Churn (Fixed column names)
key_categorical = ['Contract', 'PaymentMethod', 'InternetService', 'Dependents']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

for idx, col in enumerate(key_categorical):
    if col in df.columns:
        cross_tab = pd.crosstab(df[col], df['Churn'], normalize='index') * 100
        cross_tab.plot(kind='bar', ax=axes[idx], stacked=False)
        axes[idx].set_title(f'{col} by Churn')
        axes[idx].set_xlabel(col)
        axes[idx].set_ylabel('Percentage')
        axes[idx].legend(title='Churn')
        axes[idx].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('categorical_by_churn.png', dpi=300, bbox_inches='tight')
print("Saved: categorical_by_churn.png")
plt.close()

# Correlation matrix
corr_matrix = df[numerical_cols].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)
plt.title('Correlation Matrix - Numerical Features')
plt.tight_layout()
plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
print("Saved: correlation_matrix.png")
plt.close()

# ================================
# STEP 8: DATA CLEANING
# ================================
print("\n" + "="*50)
print("STEP 8: DATA CLEANING")
print("="*50)

# Replace 'No internet service' and 'No phone service' with 'No'
# Fixed column names (removed spaces to match raw data)
replace_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                'TechSupport', 'StreamingTV', 'StreamingMovies']

for col in replace_cols:
    if col in df.columns:
        df[col] = df[col].replace('No internet service', 'No')

if 'MultipleLines' in df.columns:
    df['MultipleLines'] = df['MultipleLines'].replace('No phone service', 'No')

print("Replaced 'No internet service' and 'No phone service' with 'No'")

# Save cleaned data
df.to_excel('Data/CustomerChurn_Cleaned.xlsx', index=False)
print("\nSaved: Data/CustomerChurn_Cleaned.xlsx")

# ================================
# STEP 9: FEATURE ENGINEERING
# ================================
print("\n" + "="*50)
print("STEP 9: FEATURE ENGINEERING")
print("="*50)

# Separate features and target
X = df.drop(columns=['Churn'])
y = df['Churn']

# Encode target variable
le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)
print(f"Target encoded: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}")

# Separate numerical and categorical features
num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_features = X.select_dtypes(include=['object']).columns.tolist()

print(f"\nNumerical features: {len(num_features)}")
print(f"Categorical features: {len(cat_features)}")

# Scale numerical features (MinMaxScaler)
scaler = MinMaxScaler()
X_num_scaled = pd.DataFrame(
    scaler.fit_transform(X[num_features]),
    columns=num_features,
    index=X.index
)
print("\nNumerical features scaled using MinMaxScaler")

# One-hot encode categorical features
X_cat_encoded = pd.get_dummies(X[cat_features], drop_first=False)
print(f"Categorical features one-hot encoded: {X_cat_encoded.shape[1]} features")

# Combine scaled numerical and encoded categorical features
X_transformed = pd.concat([X_num_scaled, X_cat_encoded], axis=1)
print(f"\nFinal transformed shape: {X_transformed.shape}")

# Add target back for saving
df_transformed = X_transformed.copy()
df_transformed['Churn'] = y_encoded

# Save transformed data
df_transformed.to_excel('Data/CustomerChurn_Transformed.xlsx', index=False)
print("\nSaved: Data/CustomerChurn_Transformed.xlsx")

# ================================
# STEP 10: FEATURE SELECTION
# ================================
print("\n" + "="*50)
print("STEP 10: FEATURE SELECTION")
print("="*50)

# Separate features and target again
X_final = df_transformed.drop(columns=['Churn'])
y_final = df_transformed['Churn']

# ANOVA F-test for numerical features
num_cols_transformed = [col for col in X_final.columns if col in num_features]
cat_cols_transformed = [col for col in X_final.columns if col not in num_features]

print(f"\nRunning ANOVA F-test on numerical features...")
selector_anova = SelectKBest(score_func=f_classif, k='all')
selector_anova.fit(X_final[num_cols_transformed], y_final)

anova_scores = pd.DataFrame({
    'Feature': num_cols_transformed,
    'Score': selector_anova.scores_
}).sort_values('Score', ascending=False)

print("\nANOVA F-test Results (Numerical Features):")
print(anova_scores)

# Chi-square test for categorical features
print(f"\nRunning Chi-square test on categorical features...")
selector_chi2 = SelectKBest(score_func=chi2, k='all')
selector_chi2.fit(X_final[cat_cols_transformed], y_final)

chi2_scores = pd.DataFrame({
    'Feature': cat_cols_transformed,
    'Score': selector_chi2.scores_
}).sort_values('Score', ascending=False)

print("\nChi-square Test Results (Categorical Features):")
print(chi2_scores)

# Visualize feature importance
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Top 15 features from ANOVA
top_anova = anova_scores.head(15)
axes[0].barh(top_anova['Feature'], top_anova['Score'])
axes[0].set_xlabel('ANOVA F-Score')
axes[0].set_title('Top Numerical Features (ANOVA F-test)')
axes[0].invert_yaxis()

# Top 15 features from Chi-square
top_chi2 = chi2_scores.head(15)
axes[1].barh(top_chi2['Feature'], top_chi2['Score'])
axes[1].set_xlabel('Chi-square Score')
axes[1].set_title('Top Categorical Features (Chi-square Test)')
axes[1].invert_yaxis()

plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
print("\nSaved: feature_importance.png")
plt.close()

# ================================
# STEP 11: FEATURE REDUCTION
# ================================
print("\n" + "="*50)
print("STEP 11: FEATURE REDUCTION")
print("="*50)

# Remove low-importance features based on analysis
# Fixed column names to match OneHotEncoder output (e.g., PhoneService_No)
features_to_remove = ['PhoneService_No', 'PhoneService_Yes',
                      'MultipleLines_No', 'MultipleLines_Yes']

df_reduced = df_transformed.drop(columns=features_to_remove, errors='ignore')
print(f"\nRemoved {len(features_to_remove)} low-importance features")
print(f"Features removed: {features_to_remove}")
print(f"\nFinal dataset shape: {df_reduced.shape}")

# Save reduced dataset
df_reduced.to_excel('Data/CustomerChurn_Transformed_Reduced.xlsx', index=False)
print("\nSaved: Data/CustomerChurn_Transformed_Reduced.xlsx")

# ================================
# STEP 12: XGBOOST WITH SHAP ANALYSIS
# ================================
print("\n" + "="*50)
print("STEP 12: XGBOOST WITH SHAP ANALYSIS")
print("="*50)

# Function to compute SHAP values using cross-validation
def compute_shap_values_cv(df, target_column='Churn', n_splits=10, use_smote=True):
    """
    Compute SHAP values using Stratified K-Fold Cross-Validation with XGBoost
    """
    # Separate features and target
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Initialize StratifiedKFold
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    # Storage for SHAP values and expected values
    all_shap_values = []
    all_expected_values = []
    all_indices = []

    # Storage for metrics
    metrics = {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'roc_auc': []
    }

    fold = 1
    for train_idx, test_idx in skf.split(X, y):
        print(f"\n{'='*20}")
        print(f"Fold {fold}:")
        print(f"{'='*20}")

        # Split data
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Apply SMOTE if requested
        if use_smote:
            smote = SMOTE(random_state=42)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            print(f"Applied SMOTE - Training set: {X_train.shape}")

        # Train XGBoost
        print("Training XGBoost...")
        model = XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            eval_metric='logloss'
        )
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        metrics['accuracy'].append(accuracy_score(y_test, y_pred))
        metrics['precision'].append(precision_score(y_test, y_pred))
        metrics['recall'].append(recall_score(y_test, y_pred))
        metrics['f1'].append(f1_score(y_test, y_pred))
        metrics['roc_auc'].append(roc_auc_score(y_test, y_pred_proba))

        print(f"Accuracy: {metrics['accuracy'][-1]:.4f}")
        print(f"Precision: {metrics['precision'][-1]:.4f}")
        print(f"Recall: {metrics['recall'][-1]:.4f}")
        print(f"F1-Score: {metrics['f1'][-1]:.4f}")
        print(f"ROC-AUC: {metrics['roc_auc'][-1]:.4f}")

        # Compute SHAP values
        print("Computing SHAP values...")
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test)

        # Store results
        all_shap_values.append(shap_values)
        # Store expected value (scalar)
        expected_val = explainer.expected_value
        all_expected_values.extend([expected_val] * len(X_test))
        all_indices.extend(test_idx)

        fold += 1

    # Combine all SHAP values
    combined_shap_values = np.vstack(all_shap_values)

    # Create DataFrames sorted by original indices
    shap_df = pd.DataFrame(combined_shap_values, columns=X.columns, index=all_indices)
    shap_df_sorted = shap_df.sort_index()

    expected_df = pd.DataFrame(all_expected_values, columns=['Expected_Value'], index=all_indices)
    expected_df_sorted = expected_df.sort_index()

    # Print average metrics
    print("\n" + "="*50)
    print("CROSS-VALIDATION RESULTS (AVERAGE)")
    print("="*50)
    for metric_name, metric_values in metrics.items():
        print(f"{metric_name.upper()}: {np.mean(metric_values):.4f} (+/- {np.std(metric_values):.4f})")

    return shap_df_sorted, expected_df_sorted, metrics

# Compute SHAP values with 10-fold CV
print("\nComputing SHAP values using 10-Fold Cross-Validation...")
shap_df_sorted, expected_values_df_sorted, cv_metrics = compute_shap_values_cv(
    df_transformed,
    target_column='Churn',
    n_splits=10,
    use_smote=True
)

# Save SHAP results
shap_df_sorted.to_csv("Results/XGBoost_Shapley_Values_10Fold_CV.csv")
expected_values_df_sorted.to_csv("Results/XGBoost_Expected_Values_10Fold_CV.csv")
print("\nSaved SHAP results to Results/ folder")

# ================================
# STEP 13: SHAP VISUALIZATIONS
# ================================
print("\n" + "="*50)
print("STEP 13: SHAP VISUALIZATIONS")
print("="*50)

# Prepare data for visualization
X_viz = df_transformed.drop(columns=['Churn'])

# 1. Beeswarm Plot (Summary Plot)
print("\nCreating Beeswarm plot...")
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_df_sorted.values, X_viz, show=False, plot_type="dot")
plt.title("SHAP Beeswarm Plot - Feature Impact on Churn Prediction", fontsize=14, pad=20)
plt.tight_layout()
plt.savefig('Results/shap_beeswarm_plot.png', dpi=300, bbox_inches='tight')
print("Saved: Results/shap_beeswarm_plot.png")
plt.close()

# 2. Bar Plot (Feature Importance)
print("Creating Bar plot...")
plt.figure(figsize=(12, 10))
shap.summary_plot(shap_df_sorted.values, X_viz, plot_type="bar", show=False)
plt.title("SHAP Bar Plot - Mean Absolute Feature Importance", fontsize=14, pad=20)
plt.tight_layout()
plt.savefig('Results/shap_bar_plot.png', dpi=300, bbox_inches='tight')
print("Saved: Results/shap_bar_plot.png")
plt.close()

# 3. Dependence Plots for Key Features
print("Creating Dependence plots...")

# MonthlyCharges (Fixed Name)
if 'MonthlyCharges' in X_viz.columns:
    plt.figure(figsize=(10, 6))
    shap.dependence_plot("MonthlyCharges", shap_df_sorted.values, X_viz, interaction_index=None, show=False)
    plt.title("SHAP Dependence Plot - MonthlyCharges", fontsize=14)
    plt.tight_layout()
    plt.savefig('Results/shap_dependence_monthly_charges.png', dpi=300, bbox_inches='tight')
    print("Saved: Results/shap_dependence_monthly_charges.png")
    plt.close()

# tenure (Fixed Name)
if 'tenure' in X_viz.columns:
    plt.figure(figsize=(10, 6))
    shap.dependence_plot("tenure", shap_df_sorted.values, X_viz, interaction_index=None, show=False)
    plt.title("SHAP Dependence Plot - Tenure", fontsize=14)
    plt.tight_layout()
    plt.savefig('Results/shap_dependence_tenure.png', dpi=300, bbox_inches='tight')
    print("Saved: Results/shap_dependence_tenure.png")
    plt.close()

# Tenure vs Contract Month-to-month (Interaction)
if 'Contract_Month-to-month' in X_viz.columns and 'tenure' in X_viz.columns:
    plt.figure(figsize=(10, 6))
    shap.dependence_plot("tenure", shap_df_sorted.values, X_viz,
                        interaction_index='Contract_Month-to-month', show=False)
    plt.title("SHAP Dependence Plot - Tenure vs Contract (Month-to-month)", fontsize=14)
    plt.tight_layout()
    plt.savefig('Results/shap_dependence_tenure_contract.png', dpi=300, bbox_inches='tight')
    print("Saved: Results/shap_dependence_tenure_contract.png")
    plt.close()

# 4. Local Explanations (Decision, Waterfall, Force Plots)
print("\nCreating local explanations for sample clients...")

# Select a few sample clients
sample_clients = [100, 250, 500]

for client_idx in sample_clients:
    # Get scalar base value
    base_val = expected_values_df_sorted.iloc[client_idx].item()

    # A. Decision Plot
    plt.figure(figsize=(10, 8))
    shap.decision_plot(
        base_val,
        shap_df_sorted.iloc[client_idx].values,
        X_viz.iloc[client_idx, :],
        link='logit'
    )
    plt.title(f"SHAP Decision Plot - Client {client_idx}", fontsize=14)
    plt.tight_layout()
    plt.savefig(f'Results/shap_decision_plot_client_{client_idx}.png', dpi=300, bbox_inches='tight')
    print(f"Saved: Results/shap_decision_plot_client_{client_idx}.png")
    plt.close()

    # B. Waterfall Plot
    exp = shap.Explanation(
        values=shap_df_sorted.iloc[client_idx].values,
        base_values=base_val,
        data=X_viz.iloc[client_idx, :],
        feature_names=X_viz.columns
    )

    plt.figure(figsize=(10, 8))
    shap.waterfall_plot(exp, max_display=15, show=False)
    plt.title(f"SHAP Waterfall Plot - Client {client_idx}", fontsize=14, pad=20)
    plt.tight_layout()
    plt.savefig(f'Results/shap_waterfall_plot_client_{client_idx}.png', dpi=300, bbox_inches='tight')
    print(f"Saved: Results/shap_waterfall_plot_client_{client_idx}.png")
    plt.close()

    # C. Force Plot (Interactive HTML)
    print(f"   Saving Force Plot for Client {client_idx}...")
    force_plot = shap.force_plot(
        base_val,
        shap_df_sorted.iloc[client_idx].values,
        X_viz.iloc[client_idx, :],
        link='logit',
        matplotlib=False
    )
    shap.save_html(f"Results/shap_force_plot_client_{client_idx}.html", force_plot)
    print(f"Saved: Results/shap_force_plot_client_{client_idx}.html")

# ================================
# STEP 14: FINAL SUMMARY
# ================================
print("\n" + "="*50)
print("STEP 14: FINAL SUMMARY")
print("="*50)

print(f"""
Complete Analysis Summary:
--------------------------
1. Original dataset: {len(df)} rows, {len(df.columns) + 2} columns (approx)
2. After cleaning: {len(df_transformed)} rows
3. After transformation: {df_transformed.shape[1]} features
4. After reduction: {df_reduced.shape[1]} features
5. Target variable: Churn (0=No, 1=Yes)
6. Churn rate: {(y_encoded.sum() / len(y_encoded)):.2%}

Cross-Validation Results (10-Fold):
----------------------------------
Accuracy:  {np.mean(cv_metrics['accuracy']):.4f} (+/- {np.std(cv_metrics['accuracy']):.4f})
Precision: {np.mean(cv_metrics['precision']):.4f} (+/- {np.std(cv_metrics['precision']):.4f})
Recall:    {np.mean(cv_metrics['recall']):.4f} (+/- {np.std(cv_metrics['recall']):.4f})
F1-Score:  {np.mean(cv_metrics['f1']):.4f} (+/- {np.std(cv_metrics['f1']):.4f})
ROC-AUC:   {np.mean(cv_metrics['roc_auc']):.4f} (+/- {np.std(cv_metrics['roc_auc']):.4f})

Files Generated:
----------------
EDA Visualizations:
1. numerical_distributions.png
2. categorical_by_churn.png
3. correlation_matrix.png
4. feature_importance.png

SHAP Visualizations:
5. shap_beeswarm_plot.png
6. shap_bar_plot.png
7. shap_dependence_monthly_charges.png
8. shap_dependence_tenure.png
9. shap_dependence_tenure_contract.png
10. shap_decision_plot_client_*.png (3 clients)
11. shap_waterfall_plot_client_*.png (3 clients)
12. shap_force_plot_client_*.html (3 clients)

Data Files:
13. Data/CustomerChurn_Cleaned.xlsx
14. Data/CustomerChurn_Transformed.xlsx
15. Data/CustomerChurn_Transformed_Reduced.xlsx
16. Results/XGBoost_Shapley_Values_10Fold_CV.csv
17. Results/XGBoost_Expected_Values_10Fold_CV.csv
""")

print("\n" + "="*50)
print("ANALYSIS COMPLETE!")
print("="*50)